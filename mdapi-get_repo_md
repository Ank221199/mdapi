#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#
# Copyright Â© 2015  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions
# of the GNU General Public License v.2, or (at your option) any later
# version.  This program is distributed in the hope that it will be
# useful, but WITHOUT ANY WARRANTY expressed or implied, including the
# implied warranties of MERCHANTABILITY or FITNESS FOR A PARTICULAR
# PURPOSE.  See the GNU General Public License for more details.  You
# should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation,
# Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
#
# Any Red Hat trademarks that are incorporated in the source
# code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission
# of Red Hat, Inc.
#

'''
This script is meant to be run in a cron job or triggered job and will pull
the sqlite database present in the repodata folder of all active Fedora
branches.

Active Fedora branches are retrieved from pkgdb2:
    https://admin.fedoraproject.org/pkgdb/

sqlite database are retrieved from the master Fedora mirror:
    https://dl.fedoraproject.org/pub/fedora/linux/

'''

import argparse
import os
import shutil
import tempfile
import time
import hashlib
import xml.etree.ElementTree as ET
import sys
import sqlite3


import requests

from fedora_messaging.api import Message, publish
from fedora_messaging.exceptions import PublishReturned, ConnectionException


DB_FOLDER = '/var/tmp'
KOJI_REPO = 'https://kojipkgs.fedoraproject.org/repos'
PKGDB2_URL = 'https://admin.fedoraproject.org/pkgdb'
DL_SERVER = 'https://dl.fedoraproject.org'
# Enforce, or not, checking the SSL certs
PKGDB2_VERIFY = True
# Valid for both koji and the download server
DL_VERIFY = True
# Whether to publish to Fedora Messaging.
PUBLISH_CHANGES = True
# How long to wait between retries if processing failed.
CRON_SLEEP = 30


repomd_xml_namespace = {
    'repo': 'http://linux.duke.edu/metadata/repo',
    'rpm': 'http://linux.duke.edu/metadata/rpm',
}

padding = 22


# Some queries to help us sort out what is in the repos
relations_query = """
    SELECT
        {table}.name,
        {table}.flags,
        {table}.epoch,
        {table}.version,
        {table}.release,
        packages.name
    FROM {table}, packages
    WHERE {table}.pkgKey == packages.pkgKey;
    """

files_query = """
    SELECT
        {table}.name,
        {table}.type,
        packages.name
    FROM {table}, packages
    WHERE {table}.pkgKey == packages.pkgKey;
    """

packages_cache_builder = """
    SELECT
        {table}.pkgId,
        {table}.name
    FROM {table};
    """

packages_query = """
    SELECT
        {table}.name,
        {table}.version,
        {table}.release,
        {table}.epoch,
        {table}.arch
    FROM {table};
    """

changelog_query = """
    SELECT
        packages.pkgId,
        {table}.author,
        {table}.date,
        {table}.changelog
    FROM {table}, packages
    WHERE {table}.pkgKey == packages.pkgKey;
    """

filelist_query = """
    SELECT
        packages.pkgId,
        {table}.dirname,
        {table}.filenames,
        {table}.filetypes
    FROM {table}, packages
    WHERE {table}.pkgKey == packages.pkgKey;
    """

default_query = "SELECT * from {table};"

queries = {
    'conflicts': relations_query,
    'enhances': relations_query,
    'obsoletes': relations_query,
    'provides': relations_query,
    'requires': relations_query,
    'supplements': relations_query,
    'recommends': relations_query,
    'suggests': relations_query,
    'files': files_query,
    'packages': packages_query,
    'changelog': changelog_query,
    'filelist': filelist_query,
}


def list_branches(status='Active'):
    ''' Return the list of Fedora branches corresponding to the given
    status.
    '''
    url = f'{PKGDB2_URL}/api/collections?clt_status={status}'
    response = requests.get(url, verify=PKGDB2_VERIFY)
    response.raise_for_status()
    data = response.json()
    return data['collections']


def download_db(name, repomd_url, archive):
    print(f'{name.ljust(padding)} Downloading file: {repomd_url} to {archive}')
    response = requests.get(repomd_url, verify=DL_VERIFY)
    response.raise_for_status()
    with open(archive, 'wb') as stream:
        stream.write(response.content)


def decompress_db(name, archive, location):
    ''' Decompress the given XZ archive at the specified location. '''
    print(f'{name.ljust(padding)} Extracting {archive} to {location}')
    if archive.endswith('.xz'):
        import lzma
        with lzma.open(archive) as inp, open(location, 'wb') as out:
            out.write(inp.read())
    elif archive.endswith('.tar.gz'):
        import tarfile
        with tarfile.open(archive) as tar:
            tar.extractall(path=location)
    elif archive.endswith('.gz'):
        import gzip
        with gzip.open(archive, 'rb') as inp, open(location, 'wb') as out:
            out.write(inp.read())
    elif archive.endswith('.bz2'):
        import bz2
        with bz2.open(archive) as inp, open(location, 'wb') as out:
            out.write(inp.read())
    else:
        raise NotImplementedError(archive)


def index_db(name, tempdb):
    print(f'{name.ljust(padding)} Indexing file: {tempdb}')

    if tempdb.endswith('primary.sqlite'):
        conn = sqlite3.connect(tempdb)
        conn.execute('CREATE INDEX packageSource ON packages (rpm_sourcerpm)')
        conn.commit()
        conn.close()


def compare_dbs(name, db1, db2, cache1, cache2):
    print(f'{name.ljust(padding)} Comparing {db1} and {db2}')

    def get_table_names(uri):
        conn = sqlite3.connect(uri)
        for name in conn.execute("SELECT name FROM sqlite_master WHERE type='table'"):
            if name[0] == 'db_info':
                continue
            yield name[0]
        conn.close()

    def row_to_package(row):
        if '/' in row[0]:
            name = row[-1]
        else:
            name = row[0]
        return name.split('(')[0]

    def get_all_rows(uri, table, cache):
        conn = sqlite3.connect(uri)
        query = queries.get(table, default_query).format(table=table)
        for i, row in enumerate(conn.execute(query)):
            if table in cache_dependant_tables:
                if row[0] in cache:
                    yield (cache[row[0]], *row[1:])
                else:
                    print(f"{name.ljust(padding)} ! {row[0]!r} does not appear in the "
                          f"{table!r} cache for {uri}. Dropping from comparison.")
            else:
                yield row
        conn.close()

    def build_cache(uri, cache):
        conn = sqlite3.connect(uri)
        query = queries.get(table, default_query).format(table=table)
        for pkgId, pkgname, *args in conn.execute(query):
            cache[pkgId] = pkgname
        conn.close()

    tables1 = list(get_table_names(db1))
    tables2 = list(get_table_names(db2))

    if not tables1 and not tables2:
        raise RuntimeError("Something is very very wrong.")

    if not tables2:
        # We have never downloaded this before...
        # so we have nothing to compare it against.  Just return and say there
        # are "no differences".
        print(f'{name.ljust(padding)} Empty! {db2} Cannot compare.')
        return set()

    assert len(tables1) == len(tables2), "Cannot compare disparate dbs."
    # These should be the same
    tables = tables1 = tables2

    # These two tables have a primary key reference to a table *in another
    # database*.  Consequently, we have to use an in-memory cache to do the
    # "join" ourselves.  Specifically, we need to swap out pkgId for pkg name.
    cache_dependant_tables = ['filelist', 'changelog']
    # This table produces the cache.
    cache_producing_tables = ['packages']

    # Prune out some squirrelly tables we're not going to worry about.
    ignored_db_tables = [
        # The 'packages' table in the 'filelists' db is full of primary keys
        # and is prone to false positives.
        ('filelists', 'packages'),
        # Same goes for the 'packages' table in the 'other' db.
        ('other', 'packages'),
    ]

    def should_compare(table):
        for test, target in ignored_db_tables:
            if test in db1 and table == target:
                return False
        return True

    tables = [table for table in tables if should_compare(table)]

    # Compare the contents of both tables and return a list of changed packages
    results = set()
    for table in tables:
        if table in cache_producing_tables:
            build_cache(db1, cache1)
            build_cache(db2, cache2)
        rows1 = set(get_all_rows(db1, table, cache1))
        rows2 = set(get_all_rows(db2, table, cache2))
        changed = rows1.symmetric_difference(rows2)
        results.update({row_to_package(row) for row in changed})

    return results


def publish_changes(name, packages, repomd_url):
    print(f'{name.ljust(padding)} Publishing differences to fedora messaging:')

    change = bool(packages)
    if not change:
        print(f'{name.ljust(padding)} No real changes.  Skipping fedora messaging.')
        return

    # Just publish the suffix of the URL.  The prefix is dl.fedoraproject.org
    # for lots of these, but we don't want to encourage people to download from
    # there.  It is the master mirror.  We want people to use
    # download.fedoraproject.org.. so, just obscure *exactly* which repo we're
    # talking about.
    url = '/'.join(repomd_url.split('/')[4:])
    print(f"{name.ljust(padding)} url {url}")

    try:
        msg = Message(
            topic="mdapi.repo.update",
            body=dict(
                name=name,
                packages=list(packages),
                url=url,
            )
        )
        publish(msg)
    except PublishReturned as e:
        print(f"Fedora Messaging broker rejected message {msg.id}: {e}",
              file=sys.stderr)
    except ConnectionException as e:
        print(f"Error sending message {msg.id}: {e}", file=sys.stderr)


def install_db(name, src, dest):
    print(f'{name.ljust(padding)} Installing {src} to {dest}.')
    shutil.move(src, dest)


def needs_update(local_file, remote_sha, sha_type):
    ''' Compare sha of a local and remote file.
    Return True if our local file needs to be updated.
    '''

    if not os.path.isfile(local_file):
        # If we have never downloaded this before, then obviously it has
        # "changed"
        return True

    # Old old epel5 doesn't even know which sha it is using..
    if sha_type == 'sha':
        sha_type = 'sha1'

    hash = getattr(hashlib, sha_type)()
    with open(local_file, 'rb') as f:
        hash.update(f.read())

    local_sha = hash.hexdigest()
    if local_sha != remote_sha:
        return True

    return False


def process_repo(repo):
    ''' Retrieve the repo metadata at the given url and store them using
    the provided name.
    '''
    url, name = repo
    repomd_url = f'{url}/repomd.xml'
    response = requests.get(repomd_url, verify=DL_VERIFY)
    if not response:
        print(f'{name.ljust(padding)} !! Failed to get {repomd_url!r} {response!r}')
        return

    # Parse the xml doc and get a list of locations and their shasum.
    files = ((
        node.find('repo:location', repomd_xml_namespace),
        node.find('repo:open-checksum', repomd_xml_namespace),
    ) for node in ET.fromstring(response.text))

    # Extract out the attributes that we're really interested in.
    files = (
        (f.attrib['href'].replace('repodata/', ''), s.text, s.attrib['type'])
        for f, s in files if f is not None and s is not None
    )

    # Filter down to only sqlite dbs
    files = ((f, s, t) for f, s, t in files if '.sqlite' in f)

    # We need to ensure the primary db comes first so we can build a pkey cache
    primary_first = lambda item: 'primary' not in item[0]
    files = sorted(files, key=primary_first)

    # Primary-key caches built from the primary dbs so we can make sense
    # of the contents of the filelist and changelog dbs.
    cache1, cache2 = {}, {}

    if not files:
        print(f'No sqlite database could be found in {url}')

    for filename, shasum, shatype in files:
        repomd_url = f'{url}/{filename}'

        # First, determine if the file has changed by comparing hash
        db = None
        if 'primary.sqlite' in filename:
            db = f'mdapi-{name}-primary.sqlite'
        elif 'filelists.sqlite' in filename:
            db = f'mdapi-{name}-filelists.sqlite'
        elif 'other.sqlite' in filename:
            db = f'mdapi-{name}-other.sqlite'

        # Have we downloaded this before?  Did it change?
        destfile = os.path.join(DB_FOLDER, db)
        if not needs_update(destfile, shasum, shatype):
            print(f'{name.ljust(padding)} No change of {repomd_url}')
            continue

        # If it has changed, then download it and move it into place.
        tempargs = dict(prefix='mdapi-', dir='/var/tmp')
        with tempfile.TemporaryDirectory(**tempargs) as working_dir:
            tempdb = os.path.join(working_dir, db)
            archive = os.path.join(working_dir, filename)

            download_db(name, repomd_url, archive)
            decompress_db(name, archive, tempdb)
            index_db(name, tempdb)
            if PUBLISH_CHANGES:
                packages = compare_dbs(name, tempdb, destfile, cache1, cache2)
                publish_changes(name, packages, repomd_url)
            else:
                print(f'{name.ljust(padding)} Not publishing to Fedora '
                      f'messaging; not comparing DBs.')
            install_db(name, tempdb, destfile)


def main():
    ''' Get the repo metadata. '''
    parser = argparse.ArgumentParser(prog="get_repo_md")
    # General connection options
    parser.add_argument('config', help="Configuration file to use")
    args = parser.parse_args()

    # Load the configuration file
    CONFIG = {}
    configfile = args.config
    with open(configfile) as config_file:
        exec(compile(
            config_file.read(), configfile, 'exec'), CONFIG)

    global DB_FOLDER
    DB_FOLDER = CONFIG.get('DB_FOLDER', DB_FOLDER)
    if not os.path.exists(DB_FOLDER):
        print('Could not find the configuration file')
        return 1

    global PKGDB2_URL, KOJI_REPO, DL_SERVER, PKGDB2_VERIFY, DL_VERIFY
    global PUBLISH_CHANGES, CRON_SLEEP
    PKGDB2_URL = CONFIG.get('PKGDB2_URL', PKGDB2_URL)
    KOJI_REPO = CONFIG.get('KOJI_REPO', KOJI_REPO)
    DL_SERVER = CONFIG.get('DL_SERVER', DL_SERVER)
    PKGDB2_VERIFY = CONFIG.get('PKGDB2_VERIFY', PKGDB2_VERIFY)
    DL_VERIFY = CONFIG.get('DL_VERIFY', DL_VERIFY)
    PUBLISH_CHANGES = CONFIG.get('PUBLISH_CHANGES', PUBLISH_CHANGES)
    CRON_SLEEP = CONFIG.get('CRON_SLEEP', CRON_SLEEP)

    if not DL_VERIFY or not PKGDB2_VERIFY:
        # Suppress urllib3's warning about insecure requests
        requests.packages.urllib3.disable_warnings()

    repositories = []
    # Get the koji repo
    repositories.append(
        (f'{KOJI_REPO}/rawhide/latest/x86_64/repodata', 'koji')
    )

    # Get the development repos (rawhide + eventually Fn+1 branched)
    dev_releases = list_branches(status='Under Development')
    for release in dev_releases:
        if release['status'] != 'Under Development':
            continue
        version = release['version']
        if version == 'devel':
            version = 'rawhide'
        url = f'{DL_SERVER}/pub/fedora/linux/development/{version}/Everything/x86_64/os/repodata'
        print(release['koji_name'], version, release['status'], url)
        repositories.append(
            (url, release['koji_name'])
        )

        url = url.replace('/x86_64/os/', '/source/tree/')
        repositories.append((url, f'src_{release["koji_name"]}'))

    urls = {
        'Fedora':
            [
                '%s/pub/fedora/linux/releases/%s/Everything/x86_64/os/repodata',
                '%s/pub/fedora/linux/updates/%s/Everything/x86_64/repodata',
                '%s/pub/fedora/linux/updates/testing/%s/Everything/x86_64/repodata',
            ],
        'Fedora EPEL':
            [
                '%s/pub/epel/%s/x86_64/repodata/',
                '%s/pub/epel/testing/%s/x86_64/repodata',
            ]
    }
    fedora_repos = ['%s', '%s-updates', '%s-updates-testing']
    epel_repos = ['%s', '%s-testing']

    # Get the stable repos
    stable_releases = list_branches(status='Active')
    for release in stable_releases:
        if release['status'] != 'Active':
            continue
        version = release['version']
        for idx, url in enumerate(urls[release['name']]):
            if release['name'] == 'Fedora':
                name = fedora_repos[idx] % release['koji_name']
            elif release['name'] == 'Fedora EPEL' and version == "8":
                name = epel_repos[idx] % release['koji_name']
                url = url.replace('/x86_64/', '/Everything/x86_64/')
            else:
                name = epel_repos[idx] % release['koji_name']
            rurl = url % (DL_SERVER, version)
            repositories.append((rurl, name))

            rurl = rurl.replace('/x86_64/os', '/source/tree')
            repositories.append((rurl, f'src_{name}'))

    # In parallel
    # p = multiprocessing.Pool(10)
    # p.map(process_repo, repositories)

    # In serial
    for repo in repositories:

        loop = True
        cnt = 0
        while loop:
            cnt += 1
            try:
                process_repo(repo)
                loop = False
            except OSError:
                if cnt == 4:
                    raise
                # Most often due to an invalid stream, so let's try a second time
                time.sleep(CRON_SLEEP)
                process_repo(repo)

    return 0


if __name__ == '__main__':
    main()
