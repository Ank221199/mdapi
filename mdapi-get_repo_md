#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#
# Copyright Â© 2015  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions
# of the GNU General Public License v.2, or (at your option) any later
# version.  This program is distributed in the hope that it will be
# useful, but WITHOUT ANY WARRANTY expressed or implied, including the
# implied warranties of MERCHANTABILITY or FITNESS FOR A PARTICULAR
# PURPOSE.  See the GNU General Public License for more details.  You
# should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation,
# Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
#
# Any Red Hat trademarks that are incorporated in the source
# code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission
# of Red Hat, Inc.
#

'''
This script is meant to be run in a cron job or triggered job and will pull
the sqlite database present in the repodata folder of all active Fedora
branches.

Active Fedora branches are retrieved from pkgdb2:
    https://admin.fedoraproject.org/pkgdb/

sqlite database are retrieved from the master Fedora mirror:
    http://dl.fedoraproject.org/pub/fedora/linux/

'''

import argparse
import contextlib
import itertools
import multiprocessing
import os
import shutil
import tempfile
import time
import hashlib
import xml.etree.ElementTree as ET
import sys


import requests

from sqlalchemy import text
from fedora_messaging.api import Message, publish
from fedora_messaging.exceptions import PublishReturned, ConnectionException

import mdapi.lib as mdapilib
import mdapi.file_lock as file_lock

KOJI_REPO = 'https://kojipkgs.fedoraproject.org/repos/'
PKGDB2_URL = 'https://admin.fedoraproject.org/pkgdb/'
DL_SERVER = 'http://dl.fedoraproject.org'
# Enforce, or not, checking the SSL certs
PKGDB2_VERIFY = True
# Valid for both koji and the download server
DL_VERIFY = True


repomd_xml_namespace = {
    'repo': 'http://linux.duke.edu/metadata/repo',
    'rpm': 'http://linux.duke.edu/metadata/rpm',
}

padding = 22


# Some queries to help us sort out what is in the repos
relations_query = """
    SELECT
        {table}.name,
        {table}.flags,
        {table}.epoch,
        {table}.version,
        {table}.release,
        packages.name
    FROM {table}, packages
    WHERE {table}.pkgKey == packages.pkgKey;
    """

files_query = """
    SELECT
        {table}.name,
        {table}.type,
        packages.name
    FROM {table}, packages
    WHERE {table}.pkgKey == packages.pkgKey;
    """

packages_cache_builder = """
    SELECT
        {table}.pkgId,
        {table}.name
    FROM {table};
    """

packages_query = """
    SELECT
        {table}.name,
        {table}.version,
        {table}.release,
        {table}.epoch,
        {table}.arch
    FROM {table};
    """

changelog_query = """
    SELECT
        packages.pkgId,
        {table}.author,
        {table}.date,
        {table}.changelog
    FROM {table}, packages
    WHERE {table}.pkgKey == packages.pkgKey;
    """

filelist_query = """
    SELECT
        packages.pkgId,
        {table}.dirname,
        {table}.filenames,
        {table}.filetypes
    FROM {table}, packages
    WHERE {table}.pkgKey == packages.pkgKey;
    """

default_query = "SELECT * from {table};"

queries = {
    'conflicts': relations_query,
    'enhances': relations_query,
    'obsoletes': relations_query,
    'provides': relations_query,
    'requires': relations_query,
    'supplements': relations_query,
    'recommends': relations_query,
    'suggests': relations_query,
    'files': files_query,
    'packages': packages_query,
    'changelog': changelog_query,
    'filelist': filelist_query,
}


def list_branches(status='Active'):
    ''' Return the list of Fedora branches corresponding to the given
    status.
    '''
    url = PKGDB2_URL + 'api/collections?clt_status=%s' % status
    response = requests.get(url, verify=PKGDB2_VERIFY)
    data = response.json()
    return data['collections']


def download_db(name, repomd_url, archive):
    print('%s Downloading file: %s to %s' % (
        name.ljust(padding), repomd_url, archive))
    response = requests.get(repomd_url, verify=DL_VERIFY)
    with open(archive, 'wb') as stream:
        stream.write(response.content)


def decompress_db(name, archive, location):
    ''' Decompress the given XZ archive at the specified location. '''
    print('%s Extracting %s to %s' % (name.ljust(padding), archive, location))
    if archive.endswith('.xz'):
        import lzma
        with contextlib.closing(lzma.LZMAFile(archive)) as stream_xz:
            data = stream_xz.read()
        with open(location, 'wb') as stream:
            stream.write(data)
    elif archive.endswith('.tar.gz'):
        import tarfile
        with tarfile.open(archive) as tar:
            tar.extractall(path=location)
    elif archive.endswith('.gz'):
        import gzip
        with open(location, 'wb') as out:
            with gzip.open(archive, 'rb') as inp:
                out.write(inp.read())
    elif archive.endswith('.bz2'):
        import bz2
        with open(location, 'wb') as out:
            bzar = bz2.BZ2File(archive)
            out.write(bzar.read())
            bzar.close()
    else:
        raise NotImplementedError(archive)


def compare_dbs(name, db1, db2, cache1, cache2):
    print('%s Comparing %s and %s' % (name.ljust(padding), db1, db2))

    def get_table_names(uri):
        with mdapilib.session_manager('sqlite:///' + uri) as session:
            for name in session.connection().engine.table_names():
                if name == 'db_info':
                    continue
                yield name

    def row_to_package(row):
        if '/' in row[0]:
            name = row[-1]
        else:
            name = row[0]
        return name.split('(')[0]

    def get_all_rows(uri, table, cache):
        query = text(queries.get(table, default_query).format(table=table))
        with mdapilib.session_manager('sqlite:///' + uri) as session:
            engine = session.connection().engine
            for i, row in enumerate(engine.execute(query)):
                if table in cache_dependant_tables:
                    row = list(row)  # lists support item assignment
                    if row[0] in cache:
                        row[0] = cache[row[0]]
                        yield tuple(row)
                    else:
                        print("%s ! %r does not appear in the "
                              "%r cache for %r.  Dropping "
                              "from comparison." % (
                                  name.ljust(padding), row[0], table, uri))
                else:
                    yield tuple(row)


    def build_cache(uri, cache):
        query = text(packages_cache_builder.format(table=table))
        with mdapilib.session_manager('sqlite:///' + uri) as session:
            engine = session.connection().engine
            for pkgId, pkgname in engine.execute(query):
                cache[pkgId] = pkgname

    tables1 = list(get_table_names(db1))
    tables2 = list(get_table_names(db2))

    if not tables1 and not tables2:
        raise RuntimeError("Something is very very wrong.")

    if not tables2:
        # We have never downloaded this before...
        # so we have nothing to compare it against.  Just return and say there
        # are "no differences".
        print('%s Empty!  %s  Cannot compare.' % (name.ljust(padding), db2))
        return set()

    assert len(tables1) == len(tables2), "Cannot compare disparate dbs."
    # These should be the same
    tables = tables1 = tables2

    # These two tables have a primary key reference to a table *in another
    # database*.  Consequently, we have to use an in-memory cache to do the
    # "join" ourselves.  Specifically, we need to swap out pkgId for pkg name.
    cache_dependant_tables = ['filelist', 'changelog']
    # This table produces the cache.
    cache_producing_tables = ['packages']

    # Prune out some squirrelly tables we're not going to worry about.
    ignored_db_tables = [
        # The 'packages' table in the 'filelists' db is full of primary keys
        # and is prone to false positives.
        ('filelists', 'packages'),
        # Same goes for the 'packages' table in the 'other' db.
        ('other', 'packages'),
    ]
    def should_compare(table):
        for test, target in ignored_db_tables:
            if test in db1 and table == target:
                return False
        return True

    tables = [table for table in tables if should_compare(table)]

    # Compare the contents of both tables and return a list of changed packages
    results = set()
    for table in tables:
        if table in cache_producing_tables:
            build_cache(db1, cache1)
            build_cache(db2, cache2)
        rows1 = set(list(get_all_rows(db1, table, cache1)))
        rows2 = set(list(get_all_rows(db2, table, cache2)))
        changed = rows1.symmetric_difference(rows2)
        results.update(set([row_to_package(row) for row in changed]))

    return results


def publish_changes(name, packages, repomd_url):
    print('%s Publishing differences to fedora messaging:' % (name.ljust(padding)))

    change = bool(packages)
    if not change:
        print('%s No real changes.  Skipping fedora messaging.' % (name.ljust(padding)))
        return

    # Just publish the suffix of the URL.  The prefix is dl.fedoraproject.org
    # for lots of these, but we don't want to encourage people to download from
    # there.  It is the master mirror.  We want people to use
    # download.fedoraproject.org.. so, just obscure *exactly* which repo we're
    # talking about.
    url = '/'.join(repomd_url.split('/')[4:])
    print("%s   url %s" % (name.ljust(padding), url))

    try:
        msg = Message(
            topic="mdapi.repo.update.v1",
            body=dict(
                name=name,
                packages=packages,
                url=url,
            )
        )
        publish(msg)
    except PublishReturned as e:
        print(
            f"Fedora Messaging broker rejected message {msg.id}: {e}", file=sys.stderr
        )
    except ConnectionException as e:
        print(f"Error sending message {msg.id}: {e}", file=sys.stderr)


def install_db(name, src, dest):
    print('%s Installing %s to %s.' % (name.ljust(padding), src, dest))
    with file_lock.FileFlock(dest + '.lock'):
        shutil.move(src, dest)


def needs_update(local_file, remote_sha, sha_type):
    ''' Compare sha of a local and remote file.
    Return True if our local file needs to be updated.
    '''

    if not os.path.isfile(local_file):
        # If we have never downloaded this before, then obviously it has
        # "changed"
        return True

    # Old old epel5 doesn't even know which sha it is using..
    if sha_type == 'sha':
        sha_type = 'sha1'

    hash = getattr(hashlib, sha_type)()
    with open(local_file, 'rb') as f:
        hash.update(f.read())

    local_sha = hash.hexdigest()
    if local_sha != remote_sha:
        return True

    return False


def process_repo(tupl):
    ''' Retrieve the repo metadata at the given url and store them using
    the provided name.
    '''
    destfolder, repo = tupl
    url, name = repo
    repomd_url = url + '/repomd.xml'
    response = requests.get(repomd_url, verify=DL_VERIFY)
    if not bool(response):
        print('%s !! Failed to get %r %r' % (
            name.ljust(padding), repomd_url, response))
        return

    # Parse the xml doc and get a list of locations and their shasum.
    files = ((
        node.find('repo:location', repomd_xml_namespace),
        node.find('repo:open-checksum', repomd_xml_namespace),
    ) for node in ET.fromstring(response.text))

    # Extract out the attributes that we're really interested in.
    files = (
        (f.attrib['href'].replace('repodata/', ''), s.text, s.attrib['type'])
        for f, s in files if f is not None and s is not None
    )

    # Filter down to only sqlite dbs
    files = ((f, s, t) for f, s, t in files if '.sqlite' in f)

    # We need to ensure the primary db comes first so we can build a pkey cache
    primary_first = lambda item: not 'primary' in item[0]
    files = sorted(files, key=primary_first)

    # Primary-key caches built from the primary dbs so we can make sense
    # of the contents of the filelist and changelog dbs.
    cache1, cache2 = {}, {}

    for filename, shasum, shatype in files:
        repomd_url = url + '/' + filename

        # First, determine if the file has changed by comparing hash
        db = None
        if 'primary.sqlite' in filename:
            db = 'mdapi-%s-primary.sqlite' % name
        elif 'filelists.sqlite' in filename:
            db = 'mdapi-%s-filelists.sqlite' % name
        elif 'other.sqlite' in filename:
            db = 'mdapi-%s-other.sqlite' % name

        # Have we downloaded this before?  Did it change?
        destfile = os.path.join(destfolder, db)
        if not needs_update(destfile, shasum, shatype):
            print('%s No change of %s' % (name.ljust(padding), repomd_url))
            continue

        # If it has changed, then download it and move it into place.
        tempargs = dict(prefix='mdapi-', dir='/var/tmp')
        with tempfile.TemporaryDirectory(**tempargs) as working_dir:
            tempdb = os.path.join(working_dir, db)
            archive = os.path.join(working_dir, filename)

            download_db(name, repomd_url, archive)
            decompress_db(name, archive, tempdb)
            packages = compare_dbs(name, tempdb, destfile, cache1, cache2)
            publish_changes(name, packages, repomd_url)
            install_db(name, tempdb, destfile)


def main():
    ''' Get the repo metadata. '''
    parser = argparse.ArgumentParser(prog="get_repo_md")
    # General connection options
    parser.add_argument('config', help="Configuration file to use")
    args = parser.parse_args()

    # Load the configuration file
    CONFIG = {}
    configfile = args.config
    with open(configfile) as config_file:
        exec(compile(
            config_file.read(), configfile, 'exec'), CONFIG)

    if not os.path.exists(CONFIG.get('DB_FOLDER', '/var/tmp')):
        print('Could not find the configuration file')
        return 1

    global PKGDB2_URL, KOJI_REPO, DL_SERVER, PKGDB2_VERIFY, DL_VERIFY
    PKGDB2_URL = CONFIG.get('PKGDB2_URL', PKGDB2_URL)
    KOJI_REPO = CONFIG.get('KOJI_REPO', KOJI_REPO)
    DL_SERVER = CONFIG.get('DL_SERVER', DL_SERVER)
    PKGDB2_VERIFY = CONFIG.get('PKGDB2_VERIFY', PKGDB2_VERIFY)
    DL_VERIFY = CONFIG.get('DL_VERIFY', DL_VERIFY)

    if not DL_VERIFY or not PKGDB2_VERIFY:
        # Suppress urllib3's warning about insecure requests
        requests.packages.urllib3.disable_warnings()

    repositories = []
    # Get the koji repo
    repositories.append(
        (KOJI_REPO + 'rawhide/latest/x86_64/repodata', 'koji')
    )

    # Get the development repos (rawhide + eventually Fn+1 branched)
    dev_releases = list_branches(status='Under Development')
    for release in dev_releases:
        if release['status'] != 'Under Development':
            continue
        version = release['version']
        if version == 'devel':
            version = 'rawhide'
        url = '%s/pub/fedora/linux/' \
            'development/%s/Everything/x86_64/os/repodata' % (DL_SERVER, version)
        repositories.append(
            (url, release['koji_name'])
        )

        url = url.replace('/x86_64/os/', '/source/tree/')
        repositories.append((url, 'src_%s' % release['koji_name']))

    urls = {
        'Fedora':
            [
                '%s/pub/fedora/linux/releases/%s/Everything/x86_64/os/repodata',
                '%s/pub/fedora/linux/updates/%s/x86_64/repodata',
                '%s/pub/fedora/linux/updates/testing/%s/x86_64/repodata',
            ],
        'Fedora EPEL':
            [
                '%s/pub/epel/%s/x86_64/repodata/',
                '%s/pub/epel/testing/%s/x86_64/repodata',
            ]
    }
    fedora_repos = ['%s', '%s-updates', '%s-updates-testing']
    epel_repos = ['%s', '%s-testing']

    # Get the stable repos
    stable_releases = list_branches(status='Active')
    for release in stable_releases:
        if release['status'] != 'Active':
            continue
        version = release['version']
        for idx, url in enumerate(urls[release['name']]):
            if release['name'] == 'Fedora':
                name = fedora_repos[idx] % release['koji_name']
            else:
                name = epel_repos[idx] % release['koji_name']
            rurl =  url % (DL_SERVER, version)
            repositories.append((rurl, name))

            rurl = rurl.replace('/x86_64/', '/SRPMS/')
            repositories.append((rurl, 'src_%s' % name))

    # In parallel
    #p = multiprocessing.Pool(10)
    #p.map(process_repo, itertools.product(
    #    [CONFIG.get('DB_FOLDER', '/var/tmp')],
    #    repositories)
    #)

    # In serial
    sleep_for = CONFIG.get('CRON_SLEEP', 30)
    for t in itertools.product(
            [CONFIG.get('DB_FOLDER', '/var/tmp')],
            repositories):

        loop = True
        cnt = 0
        while loop:
            cnt += 1
            try:
                process_repo(t)
                loop = False
            except OSError:
                if cnt == 4:
                    raise
                # Most often due to an invalid stream, so let's try a second time
                time.sleep(sleep_for)
                process_repo(t)

    return 0


if __name__ == '__main__':
    main()
