#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#
# Copyright Â© 2015  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions
# of the GNU General Public License v.2, or (at your option) any later
# version.  This program is distributed in the hope that it will be
# useful, but WITHOUT ANY WARRANTY expressed or implied, including the
# implied warranties of MERCHANTABILITY or FITNESS FOR A PARTICULAR
# PURPOSE.  See the GNU General Public License for more details.  You
# should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation,
# Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
#
# Any Red Hat trademarks that are incorporated in the source
# code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission
# of Red Hat, Inc.
#

'''
This script is meant to be run in a cron job or triggered job and will pull
the sqlite database present in the repodata folder of all active Fedora
branches.

Active Fedora branches are retrieved from pkgdb2:
    https://admin.fedoraproject.org/pkgdb/

sqlite database are retrieved from the master Fedora mirror:
    http://dl.fedoraproject.org/pub/fedora/linux/

'''

import argparse
import contextlib
import itertools
import multiprocessing
import os
import shutil
import tempfile
import hashlib

import xml.etree.ElementTree as ET

import requests

import mdapi.file_lock as file_lock

KOJI_REPO = 'https://kojipkgs.fedoraproject.org/repos/'
PKGDB2_URL = 'https://admin.fedoraproject.org/pkgdb/'

repomd_xml_namespace = {
    'repo': 'http://linux.duke.edu/metadata/repo',
    'rpm': 'http://linux.duke.edu/metadata/rpm',
}


def list_branches(status='Active'):
    ''' Return the list of Fedora branches corresponding to the given
    status.
    '''
    url = PKGDB2_URL + 'api/collections?clt_status=%s' % status
    response = requests.get(url)
    data = response.json()
    return data['collections']


def decompress_primary_db(archive, location):
    ''' Decompress the given XZ archive at the specified location. '''
    if archive.endswith('.xz'):
        import lzma
        with contextlib.closing(lzma.LZMAFile(archive)) as stream_xz:
            data = stream_xz.read()
        with file_lock.FileFlock(location + '.lock'):
            with open(location, 'wb') as stream:
                stream.write(data)
    elif archive.endswith('.gz'):
        import tarfile
        with file_lock.FileFlock(location + '.lock'):
            with tarfile.open(archive) as tar:
                tar.extractall(path=location)
    elif archive.endswith('.bz2'):
        import bz2
        with file_lock.FileFlock(location + '.lock'):
            with open(location, 'wb') as out:
                bzar = bz2.BZ2File(archive)
                out.write(bzar.read())
                bzar.close()
    elif archive.endswith('.sqlite'):
        with file_lock.FileFlock(location + '.lock'):
            with open(location, 'wb') as out:
                with open(archive) as inp:
                    out.write(inp.read())

def needs_update(local_file, remote_sha, sha_type):
    ''' Compare sha of a local and remote file.
    Return True if our local file needs to be updated.
    '''

    if not os.path.isfile(local_file):
        # If we have never downloaded this before, then obviously it has
        # "changed"
        return True

    # Old old epel5 doesn't even know which sha it is using..
    if sha_type == 'sha':
        sha_type = 'sha1'

    hash = getattr(hashlib, sha_type)()
    with open(local_file, 'rb') as f:
        hash.update(f.read())

    local_sha = hash.hexdigest()
    if local_sha != remote_sha:
        return True

    return False


def process_repo(tupl):
    ''' Retrieve the repo metadata at the given url and store them using
    the provided name.
    '''
    destfolder, repo = tupl
    url, name = repo
    repomd_url = url + '/repomd.xml'
    response = requests.get(repomd_url)
    if not bool(response):
        print("!! Failed to get %r %r" % (repomd_url, response))
        return

    # Parse the xml doc and get a list of locations and their shasum.
    files = ((
        node.find('repo:location', repomd_xml_namespace),
        node.find('repo:open-checksum', repomd_xml_namespace),
    ) for node in ET.fromstring(response.text))

    # Extract out the attributes that we're really interested in.
    files = (
        (f.attrib['href'].replace('repodata/', ''), s.text, s.attrib['type'])
        for f, s in files if f is not None and s is not None
    )

    # Filter down to only sqlite dbs
    files = ((f, s, t) for f, s, t in files if '.sqlite' in f)

    working_dir = tempfile.mkdtemp(prefix='mdapi-')
    for filename, shasum, shatype in files:
        repomd_url = url + '/' + filename

        # First, determine if the file has changed by comparing hash
        db = None
        if 'primary.sqlite' in filename:
            db = 'mdapi-%s-primary.sqlite' % name
        elif 'filelists.sqlite' in filename:
            db = 'mdapi-%s-filelists.sqlite' % name
        elif 'other.sqlite' in filename:
            db = 'mdapi-%s-other.sqlite' % name

        destfile = os.path.join(destfolder, db)
        if not needs_update(destfile, shasum, shatype):
            print('%s - No change of %s' % (name.ljust(10), repomd_url))
            continue

        # If it has changed, then download it and move it into place.
        print('%s - Downloading file: %s' % (name.ljust(10), repomd_url))
        print('%s                 to: %s' % (name.ljust(10), destfile))
        response = requests.get(repomd_url)
        archive = os.path.join(working_dir, filename)
        print(archive, filename)
        with open(archive, 'wb') as stream:
            stream.write(response.content)
        decompress_primary_db(archive, destfile)

    shutil.rmtree(working_dir)


def main():
    ''' Get the repo metadata. '''
    parser = argparse.ArgumentParser(prog="get_repo_md")
    # General connection options
    parser.add_argument('config', help="Configuration file to use")
    args = parser.parse_args()

    # Load the configuration file
    CONFIG = {}
    configfile = args.config
    with open(configfile) as config_file:
        exec(compile(
            config_file.read(), configfile, 'exec'), CONFIG)

    if not os.path.exists(CONFIG.get('DB_FOLDER', '/var/tmp')):
        print('Could not find the configuration file')
        return 1

    repositories = []
    # Get the koji repo
    repositories.append(
        (KOJI_REPO + 'rawhide/latest/x86_64/repodata', 'koji')
    )

    # Get the development repos (rawhide + eventually Fn+1 branched)
    dev_releases = list_branches(status='Under Development')
    for release in dev_releases:
        if release['status'] != 'Under Development':
            continue
        version = release['version']
        if version == 'devel':
            version = 'rawhide'
        url = 'http://dl.fedoraproject.org/pub/fedora/linux/' \
            'development/%s/x86_64/os/repodata' % version
        repositories.append(
            (url, release['koji_name'])
        )

    urls = {
        'Fedora':
            [
                'http://dl.fedoraproject.org/pub/fedora/linux/'
                'releases/%s/Everything/x86_64/os/repodata',
                'http://dl.fedoraproject.org/pub/fedora/linux/'
                'updates/%s/x86_64/repodata',
                'http://dl.fedoraproject.org/pub/fedora/linux/'
                'updates/testing/%s/x86_64/repodata',
            ],
        'Fedora EPEL':
            [
                'http://dl.fedoraproject.org/pub/epel/%s/x86_64/repodata/',
                'http://dl.fedoraproject.org/pub/epel/testing/%s/x86_64/repodata',
            ]
    }
    fedora_repos = ['%s', '%s-updates', '%s-updates-testing']
    epel_repos = ['%s', '%s-testing']

    # Get the stable repos
    stable_releases = list_branches(status='Active')
    for release in stable_releases:
        if release['status'] != 'Active':
            continue
        version = release['version']
        for idx, url in enumerate(urls[release['name']]):
            if release['name'] == 'Fedora':
                name = fedora_repos[idx] % release['koji_name']
            else:
                name = epel_repos[idx] % release['koji_name']
            rurl =  url % version
            repositories.append((rurl, name))

    p = multiprocessing.Pool(10)
    p.map(process_repo, itertools.product(
        [CONFIG.get('DB_FOLDER', '/var/tmp')],
        repositories)
    )

    return 0


if __name__ == '__main__':
    main()
